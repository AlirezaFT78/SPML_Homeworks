{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwUAmmTcl3G-"
   },
   "source": [
    "# SPML HW3: Breaking Defenses & Black-Box Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gThurMADlvnK"
   },
   "outputs": [],
   "source": [
    "name = 'ÙŽAlireza Farajtabrizi'\n",
    "std_id = '403206554'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "wnLuXjkDmJWk",
    "outputId": "68d87d61-1b0d-49f5-b1ae-854b9a34f54c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, mobilenet_v2\n",
    "from torchvision.models import ResNet18_Weights, MobileNet_V2_Weights\n",
    "from torchvision.datasets.cifar import CIFAR10\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YkpcqvmTcq6"
   },
   "source": [
    "# CIFAR10 Dataset (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iSIlVGXMoQA6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "norm_mean = (0.4914, 0.4822, 0.4465)\n",
    "norm_std = (0.2023, 0.1994, 0.2010)\n",
    "batch_size = 128\n",
    "\n",
    "mu = torch.tensor(norm_mean).view(3,1,1).to(device)\n",
    "std = torch.tensor(norm_std).view(3,1,1).to(device)\n",
    "\n",
    "# TODO: Set the upper limit and lower limit possible for images\n",
    "upper_limit = (1 - mu) / std\n",
    "lower_limit = (0 - mu) / std\n",
    "\n",
    "# Define transforms for training and testing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_3oDmJlmTD-"
   },
   "source": [
    "# Defensive Distillation (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cJsiRusm54S"
   },
   "source": [
    "[Defensive distillation](https://arxiv.org/abs/1511.04508) proceeds in four steps:\n",
    "\n",
    "1.   **Train the teacher network**, by setting the temperature of the softmax to T during the\n",
    "training phase.\n",
    "2.   **Compute soft labels** by apply the teacher network to each instance in the training set, again evaluating the softmax at temperature T.\n",
    "3.  **Train the distilled network** (a network with the same shape as the teacher network) on the soft labels, using softmax at temperature T.\n",
    "4.  Finally, when running the distilled network at test time to classify new inputs, use temperature 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppg_6w0joa-D"
   },
   "source": [
    "## Train the teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rZj3ygtFocYN"
   },
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, loss_fn, optimizer, temperature):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs) / temperature\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct_preds += (predictions == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = correct_preds / total_preds * 100\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "\n",
    "def train_teacher(model, n_epochs, loader=trainloader, temp=100):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss, epoch_accuracy = train_step(model, loader, loss_fn, optimizer, temp)\n",
    "        print(f\"Epoch [{epoch + 1}/{n_epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evuzvOUZ4bPT"
   },
   "source": [
    "You can use a pre-trained resnet to speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_oKlYe7m4w-",
    "outputId": "6f337f0c-46d1-413f-b76c-3b76a650200c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] - Loss: 1.4126, Accuracy: 47.42%\n",
      "Epoch [2/15] - Loss: 0.9707, Accuracy: 65.88%\n",
      "Epoch [3/15] - Loss: 0.7864, Accuracy: 72.52%\n",
      "Epoch [4/15] - Loss: 0.6633, Accuracy: 77.08%\n",
      "Epoch [5/15] - Loss: 0.5610, Accuracy: 80.64%\n",
      "Epoch [6/15] - Loss: 0.4732, Accuracy: 83.71%\n",
      "Epoch [7/15] - Loss: 0.3854, Accuracy: 86.77%\n",
      "Epoch [8/15] - Loss: 0.3177, Accuracy: 89.14%\n",
      "Epoch [9/15] - Loss: 0.2593, Accuracy: 90.99%\n",
      "Epoch [10/15] - Loss: 0.2082, Accuracy: 92.86%\n",
      "Epoch [11/15] - Loss: 0.1709, Accuracy: 94.10%\n",
      "Epoch [12/15] - Loss: 0.1502, Accuracy: 94.73%\n",
      "Epoch [13/15] - Loss: 0.1265, Accuracy: 95.66%\n",
      "Epoch [14/15] - Loss: 0.1102, Accuracy: 96.20%\n",
      "Epoch [15/15] - Loss: 0.0987, Accuracy: 96.56%\n"
     ]
    }
   ],
   "source": [
    "teacher = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, 10)\n",
    "teacher = teacher.to(device)\n",
    "\n",
    "train_teacher(teacher, 15)\n",
    "torch.save(teacher.state_dict(), 'Teacher.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EFhPyAmzmeB"
   },
   "source": [
    "## Test the teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iIVtvL32zopl"
   },
   "outputs": [],
   "source": [
    "def test_clean(model, dataloader=testloader):\n",
    "    model.eval()\n",
    "\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            correct_preds += (predictions == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "    accuracy = correct_preds / total_preds * 100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs9LVS7uN-Fp"
   },
   "source": [
    "Print the clean accuracy of the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tPh92LFOHG5u"
   },
   "outputs": [],
   "source": [
    "teacher = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, 10)\n",
    "\n",
    "teacher.load_state_dict(torch.load('Teacher.pth', weights_only=True))\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFEzA16Cz63A",
    "outputId": "6a0bc19b-3315-4355-98d4-5880b4d17681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Accuracy 75.99%\n"
     ]
    }
   ],
   "source": [
    "print(f'Teacher Accuracy {test_clean(teacher):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68Fnkb39vYnl"
   },
   "source": [
    "## Train the student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nfu9ZVWFvaL5"
   },
   "outputs": [],
   "source": [
    "def distill(model, teacher, dataloader, optimizer, T):\n",
    "    model.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs) / T\n",
    "            teacher_probs = F.softmax(teacher_outputs, dim=1)\n",
    "\n",
    "\n",
    "        student_outputs = model(inputs) / T\n",
    "        student_log_probs = F.log_softmax(student_outputs, dim=1)\n",
    "\n",
    "        distillation_loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean')\n",
    "        classification_loss = F.cross_entropy(student_outputs, labels)\n",
    "        total_loss = 0.9 * distillation_loss + 0.1 * classification_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        predictions = torch.argmax(student_outputs, dim=1)\n",
    "        correct_preds += (predictions == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = correct_preds / total_preds * 100\n",
    "\n",
    "    return epoch_accuracy, epoch_loss\n",
    "\n",
    "def train_student(model, teacher, n_epochs, loader=trainloader, temp=100):\n",
    "    model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        epoch_accuracy, epoch_loss = distill(model, teacher, loader, optimizer, temp)\n",
    "        print(f\"Epoch [{epoch + 1}/{n_epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80y5gpfCIJjW"
   },
   "source": [
    "This time use a `resnet18` without the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhSefyQRwcFK",
    "outputId": "a1fe4d32-c2b3-4b24-c284-3100f15fccae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] - Loss: 1.4034, Accuracy: 43.46%\n",
      "Epoch [2/15] - Loss: 0.9252, Accuracy: 64.25%\n",
      "Epoch [3/15] - Loss: 0.7368, Accuracy: 71.35%\n",
      "Epoch [4/15] - Loss: 0.6119, Accuracy: 75.92%\n",
      "Epoch [5/15] - Loss: 0.5219, Accuracy: 79.37%\n",
      "Epoch [6/15] - Loss: 0.4345, Accuracy: 82.57%\n",
      "Epoch [7/15] - Loss: 0.3652, Accuracy: 85.13%\n",
      "Epoch [8/15] - Loss: 0.2998, Accuracy: 87.57%\n",
      "Epoch [9/15] - Loss: 0.2496, Accuracy: 89.41%\n",
      "Epoch [10/15] - Loss: 0.2028, Accuracy: 91.32%\n",
      "Epoch [11/15] - Loss: 0.1734, Accuracy: 92.34%\n",
      "Epoch [12/15] - Loss: 0.1441, Accuracy: 93.61%\n",
      "Epoch [13/15] - Loss: 0.1309, Accuracy: 94.19%\n",
      "Epoch [14/15] - Loss: 0.1206, Accuracy: 94.56%\n",
      "Epoch [15/15] - Loss: 0.1085, Accuracy: 94.91%\n"
     ]
    }
   ],
   "source": [
    "student = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "student.fc = nn.Linear(student.fc.in_features, 10)\n",
    "student = student.to(device)\n",
    "\n",
    "train_student(student, teacher, 15)\n",
    "torch.save(student.state_dict(), 'Student.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snQvAMfH4ku1"
   },
   "source": [
    "## Test the student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "student.fc = nn.Linear(student.fc.in_features, 10)\n",
    "\n",
    "student.load_state_dict(torch.load('Student.pth', weights_only=True))\n",
    "student = student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_phh7vc4pA0",
    "outputId": "08d7e12d-be76-425e-f13e-f66b11396981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy 75.36%\n"
     ]
    }
   ],
   "source": [
    "print(f'Student Accuracy {test_clean(student):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdUmJKwtw-Gj"
   },
   "source": [
    "# Attack (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_-PnbtU3Zo1"
   },
   "source": [
    "Implement the FGSM attack and the `test_attack` funcion to report the robust accuracy for different values of epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "axKc8-GExDh1"
   },
   "outputs": [],
   "source": [
    "def attack_fgsm(model, x, y, epsilon, T=100):\n",
    "    x_adv = x.clone().detach().requires_grad_(True).to(device)\n",
    "    model.zero_grad()\n",
    "    outputs = model(x_adv) / T\n",
    "\n",
    "    loss = F.cross_entropy(outputs, y)\n",
    "    loss.backward()\n",
    "\n",
    "    perturbation = epsilon * x_adv.grad.sign()\n",
    "    x_adv = x_adv + perturbation\n",
    "    x_adv = torch.clamp(x_adv, lower_limit, upper_limit)\n",
    "\n",
    "    return x_adv\n",
    "\n",
    "\n",
    "def attack_pgd(model, x, y, epsilon, alpha=0.2, num_iters=10, T=100):\n",
    "    x_adv = x.clone().detach().requires_grad_(True).to(device)\n",
    "    x_orig = x.clone().detach().to(device)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        model.zero_grad()\n",
    "        outputs = model(x_adv) / T\n",
    "\n",
    "        loss = F.cross_entropy(outputs, y)\n",
    "        loss.backward()\n",
    "\n",
    "        x_adv = x_adv + alpha * x_adv.grad.sign()\n",
    "        perturbation = torch.clamp(x_adv - x_orig, -epsilon, epsilon)\n",
    "        x_adv = torch.clamp(x_orig + perturbation, lower_limit, upper_limit).detach().requires_grad_(True).to(device)\n",
    "\n",
    "    return x_adv\n",
    "\n",
    "\n",
    "def test_attack(model, epsilon, attack=attack_fgsm, loader=testloader, T=100):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_adv = attack(model, x, y, epsilon, T=T)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x_adv)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct_preds += (predictions == y).sum().item()\n",
    "            total_preds += y.size(0)\n",
    "\n",
    "    robust_accuracy = correct_preds / total_preds * 100\n",
    "\n",
    "    return robust_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqUrHA1D1Oqk"
   },
   "source": [
    "Report the robust accuracy of the teacher for `Ïµ = [1, 2, 4, 8, 16]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1HMqzS21002f",
    "outputId": "8729f7e2-a6b7-4a83-bc1a-e7917cd3a988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM with Ïµ=1/255 has Accuracy: 51.15%\n",
      "PGD  with Ïµ=1/255 has Accuracy: 49.70%\n",
      "FGSM with Ïµ=2/255 has Accuracy: 32.96%\n",
      "PGD  with Ïµ=2/255 has Accuracy: 30.80%\n",
      "FGSM with Ïµ=4/255 has Accuracy: 14.21%\n",
      "PGD  with Ïµ=4/255 has Accuracy: 12.54%\n",
      "FGSM with Ïµ=8/255 has Accuracy: 3.65%\n",
      "PGD  with Ïµ=8/255 has Accuracy: 0.31%\n",
      "FGSM with Ïµ=16/255 has Accuracy: 1.27%\n",
      "PGD  with Ïµ=16/255 has Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "epsilons = [1, 2, 4, 8, 16]\n",
    "scale = 1/std.mean().item()\n",
    "\n",
    "for eps in epsilons:\n",
    "    acc = test_attack(model=teacher, epsilon=eps*scale/255, attack=attack_fgsm, T=100)\n",
    "    print(f'FGSM with Ïµ={eps}/255 has Accuracy: {acc:.2f}%')\n",
    "    acc = test_attack(model=teacher, epsilon=eps*scale/255, attack=attack_pgd, T=100)\n",
    "    print(f'PGD  with Ïµ={eps}/255 has Accuracy: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd0E5Lrf3wyU"
   },
   "source": [
    "Do the same for the student:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bn0oMzvk3wZR",
    "outputId": "08baddf4-8fbb-4fae-c451-94d1989542f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM with Ïµ=1/255 has Accuracy: 69.25%\n",
      "PGD  with Ïµ=1/255 has Accuracy: 69.17%\n",
      "FGSM with Ïµ=2/255 has Accuracy: 69.22%\n",
      "PGD  with Ïµ=2/255 has Accuracy: 69.18%\n",
      "FGSM with Ïµ=4/255 has Accuracy: 69.21%\n",
      "PGD  with Ïµ=4/255 has Accuracy: 69.20%\n",
      "FGSM with Ïµ=8/255 has Accuracy: 69.19%\n",
      "PGD  with Ïµ=8/255 has Accuracy: 69.19%\n",
      "FGSM with Ïµ=16/255 has Accuracy: 69.22%\n",
      "PGD  with Ïµ=16/255 has Accuracy: 69.19%\n"
     ]
    }
   ],
   "source": [
    "for eps in epsilons:\n",
    "    acc = test_attack(model=student, epsilon=eps*scale/255, attack=attack_fgsm, T=1)\n",
    "    print(f'FGSM with Ïµ={eps}/255 has Accuracy: {acc:.2f}%')\n",
    "    acc = test_attack(model=student, epsilon=eps*scale/255, attack=attack_pgd, T=1)\n",
    "    print(f'PGD  with Ïµ={eps}/255 has Accuracy: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgHqlsaa33j1"
   },
   "source": [
    "What do you see?\n",
    "\n",
    "`your response:` Both FGSM and PGD attacks fails, seems like we have a defense! Increasing e won't change that. It is a sign of obfuscated gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFbxKpeM4AR9"
   },
   "source": [
    "# Transferring Adversarial Examples (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3Vu8xSsLeqN"
   },
   "source": [
    "Train yet another model to be used as the surrogate. (set temperature to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qBouBbUKHoI",
    "outputId": "e9c5b327-d1ad-488e-9d1d-45d319ebcb5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] - Loss: 2.1782, Accuracy: 25.50%\n",
      "Epoch [2/15] - Loss: 1.5049, Accuracy: 45.60%\n",
      "Epoch [3/15] - Loss: 1.2974, Accuracy: 53.38%\n",
      "Epoch [4/15] - Loss: 1.0910, Accuracy: 61.16%\n",
      "Epoch [5/15] - Loss: 0.9514, Accuracy: 66.40%\n",
      "Epoch [6/15] - Loss: 0.8288, Accuracy: 70.95%\n",
      "Epoch [7/15] - Loss: 0.7388, Accuracy: 74.02%\n",
      "Epoch [8/15] - Loss: 0.6808, Accuracy: 76.06%\n",
      "Epoch [9/15] - Loss: 0.5963, Accuracy: 79.04%\n",
      "Epoch [10/15] - Loss: 0.5322, Accuracy: 81.17%\n",
      "Epoch [11/15] - Loss: 0.4748, Accuracy: 83.38%\n",
      "Epoch [12/15] - Loss: 0.4290, Accuracy: 84.86%\n",
      "Epoch [13/15] - Loss: 0.3754, Accuracy: 86.73%\n",
      "Epoch [14/15] - Loss: 0.3300, Accuracy: 88.50%\n",
      "Epoch [15/15] - Loss: 0.2920, Accuracy: 89.65%\n"
     ]
    }
   ],
   "source": [
    "surrogate = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "surrogate.fc = nn.Linear(surrogate.fc.in_features, 10)\n",
    "surrogate = surrogate.to(device)\n",
    "\n",
    "train_teacher(surrogate, 15, temp=1)\n",
    "torch.save(surrogate.state_dict(), 'Surrogate.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WBJtNiyL8WP"
   },
   "source": [
    "Print the surrogate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oTxWsrkzKn9X"
   },
   "outputs": [],
   "source": [
    "surrogate = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "surrogate.fc = nn.Linear(surrogate.fc.in_features, 10)\n",
    "\n",
    "surrogate.load_state_dict(torch.load('Surrogate.pth', weights_only=True))\n",
    "surrogate = surrogate.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jdgr2I-VMCi3",
    "outputId": "5665cfa9-0f64-473e-9ce7-a3871df6bbce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surrogate Accuracy 72.76%\n"
     ]
    }
   ],
   "source": [
    "print(f'Surrogate Accuracy {test_clean(surrogate):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcMBV3o6MC47"
   },
   "source": [
    "Report the accuracy of the surrogate for `Ïµ = [1, 2, 4, 8, 16]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EALFUBHpMLdA",
    "outputId": "485532d0-b728-4b96-92ea-43a02fdd47ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM with Ïµ=1/255 has Accuracy: 45.56%\n",
      "FGSM with Ïµ=2/255 has Accuracy: 27.29%\n",
      "FGSM with Ïµ=4/255 has Accuracy: 10.49%\n",
      "FGSM with Ïµ=8/255 has Accuracy: 2.65%\n",
      "FGSM with Ïµ=16/255 has Accuracy: 1.57%\n"
     ]
    }
   ],
   "source": [
    "for eps in epsilons:\n",
    "    acc = test_attack(model=surrogate, epsilon=eps*scale/255, attack=attack_fgsm, T=1)\n",
    "    print(f'FGSM with Ïµ={eps}/255 has Accuracy: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buQG6pvgLsAc"
   },
   "source": [
    "Implement the following functions to transfer attacks from a surrogate model to an oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Wu2vy0kY5Rmi"
   },
   "outputs": [],
   "source": [
    "def transfer_attack(oracle, model, eps, loader=testloader):\n",
    "    oracle.eval()\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        x_adv = attack_fgsm(model, inputs, labels, eps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = oracle(x_adv)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct_preds += (predictions == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "        robust_accuracy = 100.0 * correct_preds / total_preds\n",
    "\n",
    "    return robust_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qO24bT2p6oyl"
   },
   "source": [
    "Transfer attacks for `Ïµ = [1, 2, 4, 8, 16]` from your model to the student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qfcu7QgD6vtW",
    "outputId": "4f489af3-da43-45a2-b122-f775b72249ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM with Ïµ=1/255 has Accuracy: 71.71%\n",
      "FGSM with Ïµ=2/255 has Accuracy: 68.10%\n",
      "FGSM with Ïµ=4/255 has Accuracy: 60.58%\n",
      "FGSM with Ïµ=8/255 has Accuracy: 44.87%\n",
      "FGSM with Ïµ=16/255 has Accuracy: 24.42%\n"
     ]
    }
   ],
   "source": [
    "for eps in epsilons:\n",
    "    acc = transfer_attack(student, surrogate, eps*scale/255)\n",
    "    print(f'FGSM with Ïµ={eps}/255 has Accuracy: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtGOcOmzPO3G"
   },
   "source": [
    "- What can be inferred from these results?\n",
    "- How are the accuracies of the student and the surrogate under attack related?\n",
    "- Does Defensive Distillation obfuscate the gradients? Why?\n",
    "\n",
    "`your response:`\n",
    "1. That we dont have a real defense since transfer attacks are successful and break the defense.\n",
    "Both of the accuracies decrease as epsilon increases but the transfer one falls slower.\n",
    "2. Yes it is as of the observations that we know from lectures are happening: 1-transfer attacks are successful while gradiant ones are not.\n",
    "2-increasing epsilon won't help.\n",
    "3. Using temperature = 100 forces the logits to get higher values during training phase and when we set temperature = 1 in infrence time it makes probability vectors almost one-hot which makes the gradiant 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRPjp84p81nn"
   },
   "source": [
    "# ZOO Based Black-Box Attacks (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aaXmYSbQ2W_"
   },
   "source": [
    "Based on [Black-box Adversarial Attacks with Limited Queries and Information](https://arxiv.org/abs/1804.08598) you must first calculate the estimate of the graidents, and next attack the model based on your estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Z6PlkF_A-aDS"
   },
   "outputs": [],
   "source": [
    "# Using CELoss difference for estimation\n",
    "def nes_gradient_estimate(model, x, y, epsilon, num_samples, sigma):\n",
    "    grad_estimate = torch.zeros_like(x)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            delta_i = torch.randn_like(x).to(device)\n",
    "            x_plus = x + sigma * delta_i\n",
    "            x_minus = x - sigma * delta_i\n",
    "\n",
    "            logits_plus = model(x_plus)\n",
    "            logits_minus = model(x_minus)\n",
    "            loss_plus = F.cross_entropy(logits_plus, y, reduction='none')\n",
    "            loss_minus = F.cross_entropy(logits_minus, y, reduction='none')\n",
    "            grad_estimate += (loss_plus - loss_minus).view(-1, 1, 1, 1) * delta_i\n",
    "\n",
    "    grad_estimate /= (2 * sigma * num_samples)\n",
    "    return -grad_estimate\n",
    "\n",
    "# Using Logits difference for estimation\n",
    "def nes_gradient_estimate(model, x, y, epsilon, num_samples, sigma):\n",
    "    grad_estimate = torch.zeros_like(x)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            delta_i = torch.randn_like(x).to(device)\n",
    "            x_plus = x + sigma * delta_i\n",
    "            x_minus = x - sigma * delta_i\n",
    "\n",
    "            logits_plus = model(x_plus).gather(1, y.view(-1, 1))\n",
    "            logits_minus = model(x_minus).gather(1, y.view(-1, 1))\n",
    "            grad_estimate += (logits_plus - logits_minus).view(-1, 1, 1, 1) * delta_i\n",
    "\n",
    "    grad_estimate /= (2 * sigma * num_samples)\n",
    "    return grad_estimate\n",
    "\n",
    "# Using Probabilities difference for estimation\n",
    "def nes_gradient_estimate(model, x, y, epsilon, num_samples, sigma):\n",
    "    grad_estimate = torch.zeros_like(x)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            delta_i = torch.randn_like(x).to(device)\n",
    "            x_plus = x + sigma * delta_i\n",
    "            x_minus = x - sigma * delta_i\n",
    "\n",
    "            prob_plus = F.softmax(model(x_plus), dim=1).gather(1, y.view(-1, 1))\n",
    "            prob_minus = F.softmax(model(x_minus), dim=1).gather(1, y.view(-1, 1))\n",
    "            grad_estimate += (prob_plus - prob_minus).view(-1, 1, 1, 1) * delta_i\n",
    "\n",
    "    grad_estimate /= (2 * sigma * num_samples)\n",
    "    return grad_estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used 3 different things to estimate gradiant and all of them end up almost the same result. The bottom result is made with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yBF7Spoh-nmJ"
   },
   "outputs": [],
   "source": [
    "def partial_information_attack(model, x, y, epsilon, num_samples, sigma, num_steps, alpha):\n",
    "    x_adv = x.clone().detach()\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        grad_estimate = nes_gradient_estimate(model, x_adv, y, epsilon, num_samples, sigma)\n",
    "\n",
    "        x_adv = x_adv - alpha * grad_estimate.sign()\n",
    "        x_adv = torch.clamp(x_adv, x - epsilon, x + epsilon)\n",
    "\n",
    "        x_adv = torch.clamp(x_adv, lower_limit, upper_limit)\n",
    "        x_adv = x_adv\n",
    "\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8XgGVWZRb1P"
   },
   "source": [
    "Now run this attack on your models and report the results. (You **DON'T** need to run the attack for the entire test dataset as this will take a lot of time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "N94Is59sRq8u"
   },
   "outputs": [],
   "source": [
    "def test_zoo_attack(model, epsilon, num_samples, sigma, num_steps, alpha, loader=testloader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        with torch.no_grad():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x_adv = partial_information_attack(model, x, y, epsilon, num_samples, sigma, num_steps, alpha)\n",
    "            output = model(x_adv)\n",
    "\n",
    "            predictions = output.argmax(dim=1)\n",
    "            correct_preds += (predictions == y).sum().item()\n",
    "            total_preds += y.size(0)\n",
    "\n",
    "            if total_preds > 1000:\n",
    "                break\n",
    "\n",
    "    zoo_accuracy = 100 * correct_preds / total_preds\n",
    "    return zoo_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_qAb4NADIxG",
    "outputId": "4a29418b-9eaf-4ee1-8492-a6834b819986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZOO with Ïµ=1/255 has Accuracy: 67.87%\n",
      "ZOO with Ïµ=2/255 has Accuracy: 62.30%\n",
      "ZOO with Ïµ=4/255 has Accuracy: 47.56%\n",
      "ZOO with Ïµ=8/255 has Accuracy: 15.53%\n",
      "ZOO with Ïµ=16/255 has Accuracy: 2.34%\n"
     ]
    }
   ],
   "source": [
    "epsilons = [1, 2, 4, 8, 16]\n",
    "\n",
    "for eps in epsilons:\n",
    "    acc = test_zoo_attack(model=surrogate, epsilon=eps*scale/255, num_samples=100, sigma=0.001, num_steps=10, alpha=0.1, loader=testloader)\n",
    "    print(f'ZOO with Ïµ={eps}/255 has Accuracy: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P30qgaIHVFxr"
   },
   "source": [
    "# Adversarially Robust Distillation (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlP5yndiV7Fg"
   },
   "source": [
    "In this section we are going to test another type of distillation to see if this method is robust. This technique is [Adversarially Robust Distillation](https://arxiv.org/abs/1905.09747).\n",
    "\n",
    "\n",
    "\n",
    "1.   We will try to distill a robsut teacher from [Robust Bench](https://robustbench.github.io/) onto a smaller architecture.\n",
    "2.   We minimize the KL-Divergence between the logits of the student and teacher to ensure fidelity. (You can also incorporate the classification loss as mentioned in the paper but you can choose to ignore it as well)\n",
    "3.   At each step of the distillation you will attack the student (you can use either FGSM or PGD) and find an adversarial example $X + \\delta$ for data point $X$. Next you will minimize $t^2 \\times \\text{KL}(S(X+\\delta), T(X))$ where $S$ and $T$ are the student and teacher networks respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOHVJtf0V6NT"
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/RobustBench/robustbench.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD9wjBojWz_T"
   },
   "outputs": [],
   "source": [
    "from robustbench.utils import load_model\n",
    "\n",
    "teacher = load_model(model_name='Gowal2021Improving_R18_ddpm_100m', dataset='cifar10', threat_model='Linf')\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "FLGDeE1YY6uO"
   },
   "outputs": [],
   "source": [
    "def ard(student, teacher, dataloader, optimizer, eps, attack, T, alpha):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(x) / T\n",
    "            teacher_probs = F.softmax(teacher_outputs, dim=1)\n",
    "\n",
    "        student_outputs = student(x) / T\n",
    "        classification_loss = F.cross_entropy(student_outputs, y)\n",
    "\n",
    "        x_adv = attack(student, x, y, eps, T=T)\n",
    "        student_perturbed_outputs = student(x_adv) / T\n",
    "        student_perturbed_log_probs = F.log_softmax(student_perturbed_outputs, dim=1)\n",
    "\n",
    "        distillation_loss = F.kl_div(student_perturbed_log_probs, teacher_probs, reduction='batchmean') * T * T\n",
    "        total_loss = alpha * distillation_loss + (1-alpha) * classification_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        predictions = torch.argmax(student_outputs, dim=1)\n",
    "        correct_preds += (predictions == y).sum().item()\n",
    "        total_preds += y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_accuracy = correct_preds / total_preds * 100\n",
    "\n",
    "    return epoch_accuracy, epoch_loss\n",
    "\n",
    "def adv_train_student(model, teacher, n_epochs, eps=8/255, loader=trainloader, lr=0.01, T=100, alpha=1.):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_accuracy, epoch_loss = ard(student, teacher, loader, optimizer, eps, attack_pgd, T, alpha)\n",
    "        print(f\"Epoch [{epoch + 1}/{n_epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uMsIxPr5Opku",
    "outputId": "72d0750e-da8c-4642-e362-a297ad9890d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] - Loss: 2.8294, Accuracy: 17.19%\n",
      "Epoch [2/15] - Loss: 2.3497, Accuracy: 21.36%\n",
      "Epoch [3/15] - Loss: 2.1978, Accuracy: 23.50%\n",
      "Epoch [4/15] - Loss: 2.1297, Accuracy: 25.51%\n",
      "Epoch [5/15] - Loss: 2.0597, Accuracy: 26.75%\n",
      "Epoch [6/15] - Loss: 2.0010, Accuracy: 27.47%\n",
      "Epoch [7/15] - Loss: 1.9563, Accuracy: 29.34%\n",
      "Epoch [8/15] - Loss: 1.9193, Accuracy: 30.78%\n",
      "Epoch [9/15] - Loss: 1.9044, Accuracy: 31.92%\n",
      "Epoch [10/15] - Loss: 1.8660, Accuracy: 31.20%\n",
      "Epoch [11/15] - Loss: 1.8262, Accuracy: 32.59%\n",
      "Epoch [12/15] - Loss: 1.8063, Accuracy: 34.40%\n",
      "Epoch [13/15] - Loss: 1.7858, Accuracy: 34.44%\n",
      "Epoch [14/15] - Loss: 1.7387, Accuracy: 35.12%\n",
      "Epoch [15/15] - Loss: 1.7224, Accuracy: 34.12%\n"
     ]
    }
   ],
   "source": [
    "student = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "student.classifier = nn.Linear(student.classifier[1].in_features, 10)\n",
    "student = student.to(device)\n",
    "\n",
    "adv_train_student(model=student, teacher=teacher, n_epochs=15, eps=scale*8/255, loader=trainloader, lr=0.001, T=10, alpha=0.8)\n",
    "torch.save(student.state_dict(), 'Student_Mobilenet_v2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwVcIMu9ZF12"
   },
   "source": [
    "Now report the accuracy of the student on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "student.classifier = nn.Linear(student.classifier[1].in_features, 10)\n",
    "student = student.to(device)\n",
    "\n",
    "student.load_state_dict(torch.load('Student_Mobilenet_v2.pth', weights_only=True))\n",
    "student = student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oxo-42hUtJ0",
    "outputId": "a3535a31-35cb-4e55-b936-1e81f4a32b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Accuracy 30.87%\n",
      "FGSM with Ïµ=8/255 has Accuracy: 17.19%\n",
      "PGD  with Ïµ=8/255 has Accuracy: 16.07%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Clean accurcy\n",
    "print(f'Student Accuracy {test_clean(student):.2f}%')\n",
    "\n",
    "# TODO: FGSM with eps=8/255\n",
    "acc = test_attack(model=student, epsilon=scale*8/255, attack=attack_fgsm, T=10)\n",
    "print(f'FGSM with Ïµ=8/255 has Accuracy: {acc:.2f}%')\n",
    "\n",
    "# TODO: PGD with eps=8/255\n",
    "acc = test_attack(model=student, epsilon=scale*8/255, attack=attack_pgd, T=10)\n",
    "print(f'PGD  with Ïµ=8/255 has Accuracy: {acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
