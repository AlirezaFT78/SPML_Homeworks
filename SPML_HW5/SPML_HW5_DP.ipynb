{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-family: 'Arial', sans-serif;\">\n",
    "    <h1 style=\"color: #2c3e50;\">Security and Privacy in Machine Learning</h1>\n",
    "    <h3 style=\"color: #34495e;\">Course Code: 40816</h3>\n",
    "    <h3 style=\"color: #7f8c8d;\">Instructor: Dr. Amir Mahdi Sadeghzadeh</h3>\n",
    "    <h3 style=\"color: #95a5a6;\">Sharif University of Technology, Iran</h3>\n",
    "    <h3 style=\"color: #bdc3c7;\">CE Department, Fall 2024</h3>\n",
    "    <h3 style=\"color: #d35400;\">Computer Assignment 5</h3>\n",
    "    <h4 style=\"color: #e67e22;\">Topic: Differential Privacy</h4>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'Arial', sans-serif; font-size: 14px;\">\n",
    "    <h2 style=\"color: #3498db;\">Introduction</h2>\n",
    "    <p>This notebook contains the implementation for Computer Assignment 5 on Differential Privacy. Follow the instructions carefully and ensure your code adheres to the provided guidelines.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; font-family: 'Arial', sans-serif; font-size: 14px;\">\n",
    "    <h2 style=\"color: #9b59b6;\">Personal Data</h2>\n",
    "    <p>Please enter your student number, first name, and last name in the following cell.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your personal details below\n",
    "student_number = \"\"  # Replace with your student number\n",
    "first_name = \"\"      # Replace with your first name\n",
    "last_name = \"\"       # Replace with your last name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h1 style=\"color: #2c3e50;\">Part 0: Introduction to Differential Privacy</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #34495e;\">Question 0: Differential Privacy Definitions (5 Points)</h2>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a brief summary of Differential Privacy principles. Define $\\epsilon$-differential privacy and explain its significance in protecting individual data privacy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h1 style=\"color: #2c3e50;\">Part 1: Mechanisms for Differential Privacy</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml # need sklearn >= 0.22\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide a helper function to draw bar plots from pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_pandas(series1, series2=None, label1=\"Series 1\", label2=\"Series 2\", title=\"\"):\n",
    "    '''\n",
    "    Draws a bar plot of one Pandas Series, or two pandas Series with the same index\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series1 : Series of float\n",
    "        First input\n",
    "    series2 : Series of float, optional\n",
    "        Second input (with same index)\n",
    "    label1 : string, optional\n",
    "        Label for the first series\n",
    "    label2 : string, optional\n",
    "        Label for the second series\n",
    "    title : string, optional\n",
    "        Plot title\n",
    "    '''\n",
    "    if series2 is None:\n",
    "        series1.plot.bar()\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(\"Values\")\n",
    "        plt.title(title if title else \"Bar Plot\")\n",
    "        plt.legend([label1])\n",
    "        plt.show()\n",
    "    else:\n",
    "        concat_series = pd.DataFrame({label1: series1, label2: series2}).reset_index()\n",
    "        concat_series.plot.bar(x=\"index\", y=[label1, label2], xlabel=\"\", title=title)\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(\"Values\")\n",
    "        plt.title(title if title else \"Comparison Bar Plot\")\n",
    "        plt.legend([label1, label2])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Dataset</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with a dataset from the US Census (also known as the Adult dataset). You can read about the dataset [here](https://archive.ics.uci.edu/ml/datasets/census+income).\n",
    "\n",
    "The following line loads the dataset from [OpenML](https://www.openml.org/) with the `fetch_openml` method of `sklearn`. The option `as_frame=True` (**requires sklearn version >= 0.22**) loads the dataset in `pandas DataFrame` format: this keeps the attributes in their original form and will be more convenient to work with. If you prefer working with a numpy array (not recommended), set `as_frame=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the Adult dataset from OpenML\n",
    "dataset_handle = fetch_openml(name='adult', version=2, as_frame=True)\n",
    "dataset = dataset_handle.frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the dataset, in particular the number of rows (individuals), the number of columns (attributes) and what they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the dataset\n",
    "n, d = dataset.shape  # Number of rows and columns\n",
    "print(f\"Dataset contains {n} rows (individuals) and {d} columns (attributes).\")\n",
    "print(\"\\nPreview of the dataset:\")\n",
    "print(dataset.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Question 1 (non-private queries) (7 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function for each of these types of queries (without privacy) and test them on the dataset:\n",
    "- **Simple count queries**: it takes as input a dataset (`DataFrame`), a categorical attribute (e.g., `\"sex\"`) and a value (e.g., `\"Male\"`), and returns how many rows in the dataset have the prescribed attribute value.\n",
    "- **Averaging queries**: it takes as input a dataset and a numeric attribute (e.g., `\"age\"`), and returns the average value of this attribute in the dataset.\n",
    "- **Histogram queries**: it takes as input a dataset and a categorical attribute (e.g., `\"sex\"`), and returns the histogram of counts for this attribute in the dataset (i.e., for each possible value of the attribute, how many rows have this value).\n",
    "\n",
    "Reminder: for a DataFrame `df`, we can access the column corresponding to an attribute `attr` by `df[attr]`. The method `value_counts()` allows to build a histogram of a column. To plot a pandas Series `s` as a histogram, you can use `s.plot.bar()`.\n",
    "\n",
    "Note: You can use the function `bar_plot_pandas` provided in the preamble to draw a bar plot of a pandas Series, which is useful to show histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_query(df, attribute, value):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataset\n",
    "    attribute : string\n",
    "        Name of an attribute with categorical values\n",
    "    value : string or int\n",
    "        Value of attribute to count\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    count : int\n",
    "        The number of records with `attribute=value` in dataset `df`\n",
    "    '''\n",
    "    \n",
    "    # TO COMPLETE\n",
    "    pass # this is just to avoid error while the function is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_query(df, attribute):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataset\n",
    "    attribute : string\n",
    "        Name of an attribute with numeric values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    average : float\n",
    "        The average value of `attribute` in dataset `df`\n",
    "    '''\n",
    "        \n",
    "    # TO COMPLETE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_query(df, attribute):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataset\n",
    "    attribute : string\n",
    "        Name of an attribute with categorical values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    histogram : array or Series of int\n",
    "        The histogram of `attribute`, i.e., the number of times each value of `attribute` appears in `df`\n",
    "    '''\n",
    "    # TO COMPLETE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Question 2 (Laplace mechanism) (10 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Laplace mechanism, i.e., a function which takes as input a (non-private) query output, the query's $\\ell_1$ sensitivity, the desired value of $\\epsilon$ and a random seed (for reproducibility), and returns a $\\epsilon$-differentially private estimate of the query. To draw Laplace noise, check `np.random.laplace`. The function should work with queries that output a scalar (like simple count and averaging queries), as well as those that output a vector (like histogram queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_mechanism(q, s1, eps, random_state=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : float or array/Series of float\n",
    "        The non-private output of the query\n",
    "    s1 : float\n",
    "        The L1 sensitivity of the query\n",
    "    eps : float\n",
    "        Parameter epsilon of differential privacy\n",
    "    random_state : int, optional (default=None)\n",
    "        Random seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    private_q : float or array/Series of float\n",
    "        An eps-DP evaluation of the query\n",
    "    '''\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    # TO COMPLETE\n",
    "    if hasattr(q, 'shape'): # query output is multi-dimensional\n",
    "        pass\n",
    "    else: # query output is a scalar\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Question 3 (Laplace mechanism on count queries) (8 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use the Laplace mechanism to:\n",
    "- privately count the number of males in the dataset\n",
    "- generate a private histogram of the `\"workclass\"` attribute\n",
    "\n",
    "What is the sensitivity of the query in each case?\n",
    "\n",
    "Run the Laplace mechanism with different values of $\\epsilon$ and compute the $\\ell_1$-error with respect to the true (non-private) output to see the effect on the utility and compare with the formal error bound seen in the lecture. Recall that the mechanism is random, so unless you fix the seed you will get a different result at each execution. Visually compare the private and non-private histograms using the function `bar_plot_pandas` provided at the beginning of the notebook.\n",
    "\n",
    "Note: you may round the outputs of the private mechanism to make them integers if you like. This can be considered as post-processing and thus preserves DP (see Bonus Question 5 to go further on this aspect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_error(a, b):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : float or array/Series of float\n",
    "        First input\n",
    "    b : float or array/Series of float\n",
    "        Second input\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    l1_error : float\n",
    "        The L1 distance between `a` and `b`: ||a-b||_1\n",
    "    '''\n",
    "\n",
    "    # TO COMPLETE\n",
    "    pass\n",
    "\n",
    "# list of (query,sensitivity) tuples to loop over\n",
    "# queries = \n",
    "\n",
    "# for eps in [0.1, 1.]:\n",
    "#     for q, s in queries:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Question 4 (Gaussian mechanism and comparison with Laplace) (12 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Gaussian mechanism (to draw Gaussian noise, check `np.random.normal`). Make an experimental comparison with the Laplace mechanism by plotting the privacy-utility trade-off for a 1D query of your choice. For the Gaussian mechanism, study the influence of $\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mechanism(q, s2, eps, delta, random_state=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : float or array/Series of float\n",
    "        The non-private output of the query\n",
    "    s2 : float\n",
    "        The L2 sensitivity of the query\n",
    "    eps : float\n",
    "        Parameter epsilon of differential privacy\n",
    "    delta : float\n",
    "        Parameter delta of differential privacy\n",
    "    random_state : int, optional (default=None)\n",
    "        Random seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    private_q : float or array/Series of float\n",
    "        An (eps,delta)-DP evaluation of the query\n",
    "    '''\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    # TO COMPLETE\n",
    "    if hasattr(q, 'shape'): # query output is multi-dimensional\n",
    "        pass\n",
    "    else: # query output is a scalar\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = \n",
    "# s = \n",
    "eps_list = np.linspace(0.01, 10.0, num=20)\n",
    "delta1 = 1. / n**2\n",
    "delta2 = 1. / n**4\n",
    "n_runs = 50\n",
    "\n",
    "error = np.zeros((len(eps_list), 3, n_runs))\n",
    "\n",
    "for i, eps in enumerate(eps_list):\n",
    "    for r in range(n_runs):\n",
    "        # error[i, 0, r] = \n",
    "        # error[i, 1, r] = \n",
    "        # error[i, 2, r] = \n",
    "        pass\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.errorbar(eps_list, error[:, 0, :].mean(axis=1), error[:, 0, :].std(axis=1), label='Laplace mechanism')\n",
    "ax.errorbar(eps_list, error[:, 1, :].mean(axis=1), error[:, 1, :].std(axis=1),\n",
    "            label='Gaussian mechanism ($\\delta$=' + \"{:.2e}\".format(delta1) + ')')\n",
    "ax.errorbar(eps_list, error[:, 2, :].mean(axis=1), error[:, 2, :].std(axis=1),\n",
    "            label='Gaussian mechanism ($\\delta$=' + \"{:.2e}\".format(delta2) + ')')\n",
    "plt.xlabel(\"$\\epsilon$\")\n",
    "plt.ylabel(\"$\\ell_1$ error\")\n",
    "ax.set_yscale('log')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h1 style=\"color: #2c3e50;\">Part 3: Applications of Differential Privacy</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Question 5 (contingency tables and the \"paradox\" of parallel composition) (8 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go beyond histograms and build contingency tables across two (or even more) variables. Implement a function for such queries using `crosstab` from pandas, and use the Laplace mechanism to privately find out how people in the dataset break down according to the combination of `'gender'` and `'marital-status'`. You may need to adapt your implementation of Laplace to work with k-way tables (e.g., by using the method `applymap` together with a function that adds 1D Laplace noise).\n",
    "\n",
    "Pushing this reasoning further, it may seem that there is a paradox because parallel composition gets us more information \"for free\" (e.g., we can build a k-way table over a large number of attributes that could ultimately reveal very precise information about individuals in the dataset, without increasing the privacy loss). This contradicts the principle that asking more things should cost something. Explain why there is in fact no paradox and what is the cost that one pays in this case (if needed, you can see what happens when privately releasing contingency tables across many columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_query(df, row_attr_list, col_attr_list):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Dataset\n",
    "    row_attr_list : list\n",
    "        List of attributes\n",
    "    col_attr_list : list\n",
    "        List of attributes\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    cross_table : DataFrame\n",
    "        Cross tabulation (contingency table) of `df` according to `row_attr_list` and `col_attr_list`\n",
    "    '''\n",
    "\n",
    "    # TO COMPLETE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Question 6 (Laplace mechanism on average queries) (9 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use the Laplace mechanism to privately estimate the average age of people in the dataset. We assume that the number of people in the dataset is public.\n",
    "\n",
    "1. Propose and implement simple practical strategies to compute or estimate the sensitivity of this query in the following two scenarios, and discuss the merits and/or drawbacks of your proposals:\n",
    "  - You are the trusted curator: you have access to the raw dataset and would like to release an estimate of the average age of people in the dataset with differential privacy guarantees.\n",
    "  - You are an external data analyst: you do not have access to the raw dataset but only to an API to send queries. You have to convince the trusted curator that the proposed sensitivity is safe.\n",
    "2. Suggest some ideas regarding how we could change a bit the query to get a simple and safe bound on sensitivity, at the expense of possibly introducing some bias in the output. Implement the proposed solution. Hint: the method `clip` from pandas might be useful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Question 7 (average over a subset) (6 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to compute the average age of divorced people. How can we use the Laplace mechanism to privately answer this query? Implement your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Question 8 (non-private SGD) (17 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will implement our own simple version of SGD, as well as define our own sklearn-compatible $\\ell_2$-regularized logistic regression estimator. This will be convenient when we will implement a differentially private version in Question 9.\n",
    "\n",
    "Below, you are given several pieces of code:\n",
    "1. A function `sgd` which implements SGD: it is meant to be generic in the sense that it takes as input a function `obj_and_grad` which computes the value and the gradient of the desired objective function. **This function has missing parts that you need to complete**.\n",
    "\n",
    "\n",
    "2. A function `my_logistic_obj_and_grad` (adapted from [the version from sklearn](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf39bbdacd6ed713c00724f8f871d60370/sklearn/linear_model/_logistic.py#L84)) which computes the value and gradient of the logistic regression problem. You do not need to modify this function.\n",
    "\n",
    "\n",
    "3. A class `MySGDLogisticRegression` which defines a sklearn estimator for logistic regression, where the model is fit using SGD using the previous two functions. You do not need to modify this function.\n",
    "\n",
    "Spend a bit of time to get familiar with the code provided, then complete the missing bits in the `sgd` function. Make sure it works by trying it on the binary classification dataset that you previously loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X, y, gamma, n_iter, obj_and_grad, theta_init, n_batch=1, freq_obj_eval=10,\n",
    "        n_obj_eval=1000, random_state=None):\n",
    "    \"\"\"Stochastic Gradient Descent (SGD) algorithm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n, d)\n",
    "        The data\n",
    "    y : array, shape (n,)\n",
    "        Binary labels (-1, 1).\n",
    "    gamma : float | callable\n",
    "        The step size. Can be a constant float or a function\n",
    "        that allows to have a variable step size\n",
    "    n_iter : int\n",
    "        The number of iterations\n",
    "    obj_and_grad : callable\n",
    "        A function which takes as a vector of shape (p,), a dataset of shape (n_batch, d)\n",
    "        and a label vector of shape (n_batch,), and returns the objective value and gradient.\n",
    "    theta_init : array, shape (p,)\n",
    "        The initial value for the model parameters\n",
    "    n_batch : int\n",
    "        Size of the mini-batch to use at each iteration of SGD.\n",
    "    freq_obj_eval : int\n",
    "        Specifies the frequency (in number of iterations) at which we compute the objective\n",
    "    n_obj_eval : int\n",
    "        The number of points on which we evaluate the objective\n",
    "    random_state : int\n",
    "        Random seed to make the algorithm deterministic\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta : array, shape=(p,)\n",
    "        The final value of the model parameters\n",
    "    obj_list : list of length (n_iter / freq_obj_eval)\n",
    "        A list containing the value of the objective function computed every freq_obj_eval iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n, d = X.shape\n",
    "    p = theta_init.shape[0]\n",
    "    \n",
    "    theta = theta_init.copy()\n",
    "\n",
    "    # if a constant step size was provided, we turn it into a constant function\n",
    "    if not callable(gamma):\n",
    "        def gamma_func(t):\n",
    "            return gamma\n",
    "    else:\n",
    "        gamma_func = gamma\n",
    "    \n",
    "    # list to record the evolution of the objective (for plotting)\n",
    "    obj_list = []\n",
    "    # we draw a fixed subset of points to monitor the objective\n",
    "    idx_eval = rng.randint(0, n, n_obj_eval)\n",
    "\n",
    "    for t in range(n_iter):\n",
    "        if t % freq_obj_eval == 0:\n",
    "            # evaluate objective\n",
    "            obj, _ = obj_and_grad(theta, X[idx_eval, :], y[idx_eval])\n",
    "            obj_list.append(obj)\n",
    "        \n",
    "        # TO COMPLETE\n",
    "        \n",
    "    return theta, obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model._base import LinearClassifierMixin, SparseCoefMixin, BaseEstimator\n",
    "from sklearn.utils.extmath import log_logistic, safe_sparse_dot\n",
    "from sklearn.linear_model._logistic import _intercept_dot\n",
    "from scipy.special import expit\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "def my_logistic_obj_and_grad(theta, X, y, lamb):\n",
    "    \"\"\"Computes the value and gradient of the objective function of logistic regression defined as:\n",
    "    min (1/n) \\sum_i log_loss(theta;X[i,:],y[i]) + (lamb / 2) \\|w\\|^2,\n",
    "    where theta = w (if no intercept), or theta = [w b] (if intercept)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_init : array, shape (d,) or (d+1,)\n",
    "        The initial value for the model parameters. When an intercept is used, it corresponds to the last entry\n",
    "    X : array, shape (n, d)\n",
    "        The data\n",
    "    y : array, shape (n,)\n",
    "        Binary labels (-1, 1)\n",
    "    lamb : float\n",
    "        The L2 regularization parameter\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    obj : float\n",
    "        The value of the objective function\n",
    "    grad : array, shape (d,) or (d+1,)\n",
    "        The gradient of the objective function\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    grad = np.empty_like(theta)\n",
    "\n",
    "    w, c, yz = _intercept_dot(theta, X, y)\n",
    "\n",
    "    # Logistic loss is the negative of the log of the logistic function\n",
    "    obj = -np.mean(log_logistic(yz)) + .5 * lamb * np.dot(w, w)\n",
    "\n",
    "    z = expit(yz)\n",
    "    z0 = (z - 1) * y\n",
    "\n",
    "    grad[:n_features] = safe_sparse_dot(X.T, z0) / n_samples + lamb * w\n",
    "\n",
    "    # Case where we fit the intercept\n",
    "    if grad.shape[0] > n_features:\n",
    "        grad[-1] = z0.sum() / n_samples\n",
    "    return obj, grad\n",
    "\n",
    "\n",
    "class MySGDLogisticRegression(BaseEstimator, LinearClassifierMixin, SparseCoefMixin):\n",
    "    \"\"\"Our own sklearn estimator for logistic regression defined as:\n",
    "    min (1/n) \\sum_i log_loss(theta;X[i,:],y[i]) + (lamb / 2) \\|w\\|^2,\n",
    "    where theta = [w b]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float | callable\n",
    "        The step size. Can be a constant float or a function\n",
    "        that allows to have a variable step size\n",
    "    n_iter : int\n",
    "        The number of iterations\n",
    "    lamb : float\n",
    "        The L2 regularization parameter\n",
    "    n_batch : int\n",
    "        Size of the mini-batch to use at each iteration of SGD.\n",
    "    freq_obj_eval : int\n",
    "        Specifies the frequency (in number of iterations) at which we compute the objective\n",
    "    n_obj_eval : int\n",
    "        The number of points on which we evaluate the objectuve\n",
    "    random_state : int\n",
    "        Random seed to make the algorithm deterministic\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : (p,)\n",
    "        The weights of the logistic regression model.\n",
    "    intercept_ : (1,)\n",
    "        The intercept term of the logistic regression model.\n",
    "    obj_list_: list of length (n_iter / freq_obj_eval)\n",
    "        A list containing the value of the objective function computed every freq_loss_eval iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma, n_iter, lamb=0, n_batch=1, freq_obj_eval=10, n_obj_eval=1000, random_state=None):\n",
    "        self.gamma = gamma\n",
    "        self.n_iter = n_iter\n",
    "        self.lamb = lamb\n",
    "        self.n_batch = n_batch\n",
    "        self.freq_obj_eval = freq_obj_eval\n",
    "        self.n_obj_eval = n_obj_eval\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # WARNING: assumes labels are -1, 1\n",
    "        X, y = check_X_y(X, y, accept_sparse='csr', dtype=[np.float64, np.float32], order=\"C\")\n",
    "        self.classes_ = np.unique(y)    \n",
    "                \n",
    "        p = X.shape[1]\n",
    "        theta_init = np.zeros(p+1) # initialize parameters to zero\n",
    "        # define the function for value and gradient needed by SGD\n",
    "        obj_grad = lambda theta, X, y: my_logistic_obj_and_grad(theta, X, y, lamb=self.lamb)\n",
    "        theta, obj_list = sgd(X, y, self.gamma, self.n_iter, obj_grad, theta_init, self.n_batch,\n",
    "                              self.freq_obj_eval, self.n_obj_eval, self.random_state)\n",
    "        \n",
    "        # save the learned model into the appropriate quantities used by sklearn\n",
    "        self.intercept_ = np.expand_dims(theta[-1], axis=0)\n",
    "        self.coef_ = np.expand_dims(theta[:-1], axis=0)\n",
    "        \n",
    "        # also save list of objective values during optimization for plotting\n",
    "        self.obj_list_ = obj_list\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = 0\n",
    "n_iter = 5000\n",
    "n_batch = 1\n",
    "gamma = 0.05\n",
    "# gamma = lambda t: 1 / np.sqrt(t)\n",
    "\n",
    "mlr = MySGDLogisticRegression(gamma, n_iter, lamb, n_batch=n_batch, random_state=None)\n",
    "mlr.fit(X_train, y_train)\n",
    "print(\"Test accuracy\", mlr.score(X_test, y_test))\n",
    "\n",
    "obj_list = mlr.obj_list_\n",
    "iter_list = np.arange(len(obj_list)) * mlr.freq_obj_eval\n",
    "plt.plot(iter_list, obj_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Question 9 (private SGD) (18 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement and experiment with DP-SGD:\n",
    "1. Following the model of the function `sgd`, implement a new function `private_sgd` which implements DP-SGD **with mini-batch size of 1 and no regularization**. It can take as input the desired value of $\\epsilon$ and $\\delta$ for the $(\\epsilon,\\delta)$-DP, or alternatively the standard deviation of the Gaussian noise to add at each iteration. Note: you do not need to make the objective plotting part private (this is only for monitoring).\n",
    "\n",
    "\n",
    "2. Following the model of the class `MySGDLogisticRegression`, implement a new class `MyPrivateSGDLogisticRegression` which implements differentially private logistic regression trained using your DP-SGD implementation above.\n",
    "\n",
    "\n",
    "3. Experiment with different values of $\\epsilon$ and $\\delta$, number of iterations and step size, and study the effect on the convergence of SGD as well as the test accuracy of the resulting model. Describe your observations. How should the number of iterations depend on the level of privacy? How can we choose the number of iterations and step size in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: font-family: 'Arial', sans-serif;\">\n",
    "    <h2 style=\"color: #2c3e50;\">Optional: Question 10 (federated learning with DP: DP-SGD in the distributed model) (20 Points)</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough with these simple queries! Let's train a machine learning model in the federated learning, in which $n$ participants with their own datasets collaborate to train a joint model. Each participant $i$ wants to ensure that the algorithm satisfies $(\\epsilon,\\delta)$-DP with respect to his/her own dataset $D_i$. This is sometimes referred to as the distributed model of DP. Note that if each participant has a dataset of size 1 then this is exactly local DP. However, the privacy-utility trade-off will be better when participants have more data points, which is what we consider below.\n",
    "\n",
    "The following code loads the US Census dataset in one-hot encoded version. Feel free to use another binary classification dataset of your choice instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(name='a9a', version=1, return_X_y=True, as_frame=False)\n",
    "normalizer = Normalizer()\n",
    "X = normalizer.transform(X)\n",
    "m, d = X.shape\n",
    "print(m, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a setting with $n=5$ participants. To simulate the federated learning setting, we will split the dataset in $n$ local datasets of roughly equal size. To do this, we use `sklearn.model_selection.KFold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "features = {}\n",
    "labels = {}\n",
    "for i, idx in enumerate(KFold(n_splits=n, shuffle=True).split(X)):\n",
    "    features[i] = X[idx[1],:]\n",
    "    labels[i] = y[idx[1]]\n",
    "    \n",
    "for i in range(n):\n",
    "    print(\"Dataset of participant \" + str(i) + \":\", features[i].shape, labels[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now like to train a logistic regression classifier with DP-SGD in the federated setting. For simplicity of exposition, assume the presence of an *untrusted* aggregator. The algorithm follows an interative process, where each iteration consists of the following steps:\n",
    "1. The trusted aggregator sends the current parameters of the model to the participants.\n",
    "\n",
    "2. Each participant $i$ computes a stochastic gradient using a mini-batch from his local dataset $D_i$, adds Gaussian noise locally to ensure DP, and sends it to the untrusted aggregator.\n",
    "\n",
    "3. The untrusted aggregator averages these gradients and use the result to update the model with a gradient step.\n",
    "\n",
    "How much Gaussian noise should each participant add at each iteration to ensure an $(\\epsilon,\\delta)$-DP guarantee for the entire algorithm?\n",
    "\n",
    "Adapt your centralized DP-SGD code from the previous practical to simulate this federated learning version. Compare the utility with the centralized version, studying in particular the effect of the number of participants.\n",
    "\n",
    "Suppose that the local dataset sizes are imbalanced across participants. How does this affect the Gaussian noise added by each participant? How does this effect the utility? Suggest an appropriate weighted aggregation scheme to mitigate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
