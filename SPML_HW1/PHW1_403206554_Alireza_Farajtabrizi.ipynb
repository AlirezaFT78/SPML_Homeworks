{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooJtNr5dGrH9"
   },
   "source": [
    "**Name:** Alireza Farajtabrizi  \n",
    "\n",
    "**Student Number:** 403206554\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJm9Z1k0cdmh"
   },
   "source": [
    "# Neural-Network with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDN075MYGesD"
   },
   "source": [
    "In this notebook, you are going to write and implement all the components required to create and train a two-layered neural network using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt3FdxgNcdmm"
   },
   "source": [
    "## Imports & Seeding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPZ4zlnxqhl5"
   },
   "source": [
    "Importing some common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Et7OS7TGcdmn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fa2v2-xbcdmo"
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKWqV2Gycdmp"
   },
   "source": [
    "You'll train and evaluate your model on [Fashion MNIST](https://en.wikipedia.org/wiki/Fashion_MNIST) dataset. In this section, you'll download Fashion MNIST and split it into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tMYZtSoLc7c-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Using `fetch_openml`, download `Fashion-MNIST` \n",
    "# and save the training data and labels in `X` and `y` respectively.\n",
    "#############################\n",
    "# Your code goes here (5 points)\n",
    "fashion_mnist = fetch_openml('Fashion-MNIST', version=1)\n",
    "X = fashion_mnist.data\n",
    "y = fashion_mnist.target    \n",
    "#############################\n",
    "\n",
    "# Normalization:\n",
    "X = ((X / 255.) - .5) * 2\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sDmxyMJ4dBk3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using `train_test_split`, split your data into two sets. \n",
    "# Set the test_size to 10000\n",
    "\n",
    "#############################\n",
    "# Your code goes here (6 points)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
    "#############################\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiGTXGXKcdmt"
   },
   "source": [
    "## Prepare training & validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba3nNYlDcdmt"
   },
   "source": [
    "We'll use only 3 classes from Fashion MNIST: Trouser, T-shirt, and Sneaker classes.\n",
    "\n",
    "The class labels for T-shirt, Trouser, and Sneaker are 0, 1, and 7 respectively.\n",
    "\n",
    "In this part, you'll limit the testing and training sets to only these three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TcBDZEtzcdmu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18022, 784) (18022,)\n"
     ]
    }
   ],
   "source": [
    "# Modify `y_train` and `x_train`.\n",
    "# Only keep the 3 classes mentioned above. \n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "classes_to_keep = ['0', '1', '7']\n",
    "\n",
    "mask = np.isin(y_train, classes_to_keep)\n",
    "\n",
    "x_train = x_train[mask]\n",
    "y_train = y_train[mask]\n",
    "y_train = np.array(y_train, dtype=int)\n",
    "y_train[y_train==7]=2\n",
    "#############################\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LX2hkRe1cdmw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2978, 784) (2978,)\n"
     ]
    }
   ],
   "source": [
    "# Modify `y_test` and `x_test`.\n",
    "# Only keep the 3 classes mentioned above. \n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "classes_to_keep = ['0', '1', '7']\n",
    "\n",
    "mask = np.isin(y_test, classes_to_keep)\n",
    "\n",
    "x_test = x_test[mask]\n",
    "y_test = y_test[mask]\n",
    "y_test = np.array(y_test, dtype=int)\n",
    "y_test[y_test==7]=2\n",
    "#############################\n",
    "\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv6SMLUktWbv"
   },
   "source": [
    "## Linear & Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXlyJo5JteKC"
   },
   "source": [
    "In this part, you'll implement the forward and backward process for the following components:\n",
    "- Softmax Layer\n",
    "- Linear Layer\n",
    "- ReLU Layer\n",
    "- Sigmoid Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXtAD5uYA4sQ"
   },
   "source": [
    "### The `Softmax` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tzaIVo-_Axp7"
   },
   "outputs": [],
   "source": [
    "class SoftMaxLayer(object):\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Write the forward pass for softmax.\n",
    "        # Save the values required for the backward pass.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        self.inp = x\n",
    "        exp_x = np.exp(x)\n",
    "        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        #############################\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, up_grad):\n",
    "        # Write the backward pass for softmax.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        batch_size = self.output.shape[0]\n",
    "        grad = np.zeros_like(up_grad)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            y = self.output[i].reshape(-1, 1)\n",
    "            jacobian = np.diagflat(y) - np.dot(y, y.T)\n",
    "            grad[i] = np.dot(jacobian, up_grad[i])\n",
    "            \n",
    "        return grad\n",
    "        #############################\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcFoIDZjcdnB"
   },
   "source": [
    "### The `Linear` Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1strsTh6cdnG"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        # Initialize the layer's weights and biases\n",
    "        #############################\n",
    "        # Your code goes here (2 points)\n",
    "        self.w = np.random.randn(out_dim, in_dim) * 0.1\n",
    "        self.b = np.zeros((1, out_dim))  \n",
    "        #############################\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # Compute linear layer's output.\n",
    "        # Save the value(s) required for the backward phase.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        self.inp = inp  \n",
    "        z = np.dot(self.inp, self.w.T) + self.b \n",
    "        #############################\n",
    "        return z\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        # Calculate the gradient with respect to the weights \n",
    "        # and biases and save the results.\n",
    "        #############################\n",
    "        # Your code goes here (6 points)\n",
    "        self.dw = np.dot(up_grad.T, self.inp)\n",
    "        self.db = np.sum(up_grad, axis=0, keepdims=True)\n",
    "        down_grad = np.dot(up_grad, self.w)\n",
    "        #############################\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        # Update the layer's weights and biases\n",
    "        # Update previous_w_update and previous_b_update accordingly\n",
    "        #############################\n",
    "        # Your code goes here (5 points)]\n",
    "        self.previous_w_update = self.w\n",
    "        self.previous_b_update = self.b\n",
    "\n",
    "        self.w = optimizer.get_next_update(self.w, self.dw)\n",
    "        self.b = optimizer.get_next_update(self.b, self.db)\n",
    "        \n",
    "        self.previous_w_update = self.w - self.previous_w_update\n",
    "        self.previous_b_update = self.b - self.previous_b_update\n",
    "        #############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0Lfo-nhcdnG"
   },
   "source": [
    "### The `ReLU` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tN6vcirMcdnH"
   },
   "outputs": [],
   "source": [
    "class RelU:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Write the forward pass for ReLU.\n",
    "        # Save the value(s) required for the backward pass.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        self.inp = inp\n",
    "\n",
    "        output = np.maximum(0, inp)\n",
    "        #############################\n",
    "        return output\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        down_grad = np.zeros_like(up_grad)\n",
    "        down_grad[self.inp > 0] = up_grad[self.inp > 0]\n",
    "        #############################\n",
    "        return down_grad\n",
    "\n",
    "    def step(self, optimizer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z00KoSI3cdnJ"
   },
   "source": [
    "### The `sigmoid` Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TTYYeL2lcdnJ"
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self, inp):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        self.out = 1 / (1 + np.exp(-inp))\n",
    "        #############################\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        down_grad = up_grad * (self.out * (1 - self.out))\n",
    "        #############################\n",
    "        return down_grad\n",
    "    \n",
    "    def step(self, optimizer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zngleGY2cdnK"
   },
   "source": [
    "## `Loss` function :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISedT4FvcdnK"
   },
   "source": [
    "For this task we are going to use the [Cross-Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XQyz4ybycdnL"
   },
   "outputs": [],
   "source": [
    "class CELoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \n",
    "        self.yhat = pred\n",
    "        self.y = target\n",
    "        m = self.y.shape[0]\n",
    "        # Commpute and return the loss \n",
    "        #############################\n",
    "        # Your code goes here (8 points)\n",
    "        gathered_probs = pred[self.y==1]\n",
    "        loss = -np.mean(np.log(gathered_probs + 1e-15))\n",
    "        return loss\n",
    "        #############################\n",
    "        \n",
    "    \n",
    "    def backward(self):\n",
    "        # Derivative of loss_fn with respect to a the predicted label.\n",
    "        # Use `self.y` and `self.yhat` to compute and return `grad`.\n",
    "        #############################\n",
    "        # Your code goes here (6 points)\n",
    "        m = self.y.shape[0]\n",
    "        grad = np.zeros_like(self.yhat)\n",
    "        \n",
    "        grad[self.y==1] = -1 / (self.yhat[self.y==1] + 1e-15)\n",
    "        grad = grad / m\n",
    "        #############################\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xovZI-70kB9I"
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "In this section, you'll implement an optimizer classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def get_next_update(self, x, dx):\n",
    "        # Compute the new value for 'x' and return the result\n",
    "        #############################\n",
    "        # Your code goes here (2 points)\n",
    "        x = x - self.lr * dx\n",
    "        return x\n",
    "        #############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxxrEEovYEFi"
   },
   "source": [
    "## The Model\n",
    "Now you'll write the base class for a multi-layer perceptron network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "t8SoZeYRcdnY"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers, loss_fn, optimizer):\n",
    "        self.layers = layers \n",
    "        self.losses  = [] \n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Pass `inp` to all the layers sequentially\n",
    "        # and return the result.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp)\n",
    "        return inp\n",
    "        #############################\n",
    "        \n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        loss = self.loss_fn.forward(pred, label)\n",
    "        self.losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        # Start with loss function's gradient and \n",
    "        # do the backward pass on all the layers.\n",
    "        #############################\n",
    "        # Your code goes here (5 points)\n",
    "        grad = self.loss_fn.backward()  \n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        #############################\n",
    "        \n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            layer.step(self.optimizer)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo0rNwYciueF"
   },
   "source": [
    "The following cell encodes training labels into a one-hot representation with 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nhJTulaFJ4vR"
   },
   "outputs": [],
   "source": [
    "def onehot_enc(y, num_labels):\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, int(val)] = 1\n",
    "    return ary\n",
    "\n",
    "y_train = onehot_enc(y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TS6S_RUwsRkF"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, x, y):\n",
    "    for n in range(epochs):\n",
    "        # First do the forward pass. Next, compute the loss.\n",
    "        # Then do the backward pass and finally, update the parameters.\n",
    "        #############################\n",
    "        # Your code goes here (4 points)\n",
    "        y_pred = model.forward(x)\n",
    "        loss = model.loss(y_pred, y)\n",
    "        model.backward()\n",
    "        model.update()\n",
    "        #############################\n",
    "        print(f\"Loss at {n}: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "m1lSq2jNcdnY",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 0: 1.104\n",
      "Loss at 1: 1.077\n",
      "Loss at 2: 1.051\n",
      "Loss at 3: 1.027\n",
      "Loss at 4: 1.004\n",
      "Loss at 5: 0.981\n",
      "Loss at 6: 0.958\n",
      "Loss at 7: 0.934\n",
      "Loss at 8: 0.910\n",
      "Loss at 9: 0.886\n",
      "Loss at 10: 0.863\n",
      "Loss at 11: 0.842\n",
      "Loss at 12: 0.822\n",
      "Loss at 13: 0.804\n",
      "Loss at 14: 0.788\n",
      "Loss at 15: 0.774\n",
      "Loss at 16: 0.761\n",
      "Loss at 17: 0.749\n",
      "Loss at 18: 0.738\n",
      "Loss at 19: 0.728\n",
      "Loss at 20: 0.719\n",
      "Loss at 21: 0.711\n",
      "Loss at 22: 0.704\n",
      "Loss at 23: 0.697\n",
      "Loss at 24: 0.691\n",
      "Loss at 25: 0.685\n",
      "Loss at 26: 0.680\n",
      "Loss at 27: 0.675\n",
      "Loss at 28: 0.670\n",
      "Loss at 29: 0.666\n",
      "Loss at 30: 0.662\n",
      "Loss at 31: 0.658\n",
      "Loss at 32: 0.655\n",
      "Loss at 33: 0.652\n",
      "Loss at 34: 0.649\n",
      "Loss at 35: 0.646\n",
      "Loss at 36: 0.643\n",
      "Loss at 37: 0.641\n",
      "Loss at 38: 0.638\n",
      "Loss at 39: 0.636\n",
      "Loss at 40: 0.634\n",
      "Loss at 41: 0.632\n",
      "Loss at 42: 0.630\n",
      "Loss at 43: 0.628\n",
      "Loss at 44: 0.626\n",
      "Loss at 45: 0.625\n",
      "Loss at 46: 0.623\n",
      "Loss at 47: 0.622\n",
      "Loss at 48: 0.620\n",
      "Loss at 49: 0.619\n",
      "Loss at 50: 0.618\n",
      "Loss at 51: 0.617\n",
      "Loss at 52: 0.615\n",
      "Loss at 53: 0.614\n",
      "Loss at 54: 0.613\n",
      "Loss at 55: 0.612\n",
      "Loss at 56: 0.611\n",
      "Loss at 57: 0.610\n",
      "Loss at 58: 0.609\n",
      "Loss at 59: 0.608\n",
      "Loss at 60: 0.608\n",
      "Loss at 61: 0.607\n",
      "Loss at 62: 0.606\n",
      "Loss at 63: 0.605\n",
      "Loss at 64: 0.605\n",
      "Loss at 65: 0.604\n",
      "Loss at 66: 0.603\n",
      "Loss at 67: 0.603\n",
      "Loss at 68: 0.602\n",
      "Loss at 69: 0.601\n",
      "Loss at 70: 0.601\n",
      "Loss at 71: 0.600\n",
      "Loss at 72: 0.600\n",
      "Loss at 73: 0.599\n",
      "Loss at 74: 0.599\n",
      "Loss at 75: 0.598\n",
      "Loss at 76: 0.598\n",
      "Loss at 77: 0.597\n",
      "Loss at 78: 0.597\n",
      "Loss at 79: 0.596\n",
      "Loss at 80: 0.596\n",
      "Loss at 81: 0.595\n",
      "Loss at 82: 0.595\n",
      "Loss at 83: 0.595\n",
      "Loss at 84: 0.594\n",
      "Loss at 85: 0.594\n",
      "Loss at 86: 0.594\n",
      "Loss at 87: 0.593\n",
      "Loss at 88: 0.593\n",
      "Loss at 89: 0.593\n",
      "Loss at 90: 0.592\n",
      "Loss at 91: 0.592\n",
      "Loss at 92: 0.592\n",
      "Loss at 93: 0.591\n",
      "Loss at 94: 0.591\n",
      "Loss at 95: 0.591\n",
      "Loss at 96: 0.590\n",
      "Loss at 97: 0.590\n",
      "Loss at 98: 0.590\n",
      "Loss at 99: 0.590\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the `MLP` with the following structure:\n",
    "#     Linear with 50 units --> ReLU --> Linear with 50 units --> ReLU --> Linear with 3 units --> Sigmoid --> Softmax\n",
    "# Use GradientDescent as the optimizer, set the learning rate to 0.001, and use CELoss as the loss function.\n",
    "#############################\n",
    "# Your code goes here (4 points)\n",
    "np.random.seed(42)\n",
    "layers = [\n",
    "            Linear(x_train.shape[1],50),\n",
    "            RelU(),\n",
    "            Linear(50,50),\n",
    "            RelU(),\n",
    "            Linear(50,3),\n",
    "            Sigmoid(),\n",
    "            SoftMaxLayer()]\n",
    "\n",
    "nn = MLP(layers, CELoss(), GradientDescent(0.1))\n",
    "#############################\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Train the network using only `x_train` and `y_train` (no validation)\n",
    "train(nn, epochs, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJec2xRJmY37"
   },
   "source": [
    "Let's plot the loss value for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ymaQNn70cdnZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses)\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dz3yqRa1cdna"
   },
   "source": [
    "**Let's also check our model's performance using the `accuracy` metric on the `testing` dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TRqwXho7cdnd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9754869039623909\n"
     ]
    }
   ],
   "source": [
    "# Compute the accuracy on the testing set\n",
    "#############################\n",
    "# Your code goes here (7 points)\n",
    "y_test_pred = nn.forward(x_test).argmax(axis=1)\n",
    "acc = np.count_nonzero(y_test_pred == y_test)/len(y_test)\n",
    "#############################\n",
    "\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
